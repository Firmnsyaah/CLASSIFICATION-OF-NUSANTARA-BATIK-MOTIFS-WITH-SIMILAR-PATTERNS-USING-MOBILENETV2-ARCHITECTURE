# -*- coding: utf-8 -*-
"""Klasifikasi Citra Motif Batik Nusantara.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13tWVMio_dfE7bx7CrbLA1h5WUczmJnbv
"""

import tensorflow as tf

print(tf.__version__)

physical_devices = tf.config.list_physical_devices('GPU')
print("GPUs Available: ", physical_devices)

if physical_devices:
    try:
        for gpu in physical_devices:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

import os
import tensorflow as tf
import keras
import numpy as np
import matplotlib.pyplot as plt

# Import dari tensorflow/keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

"""#Kelas"""

import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import Input


# Menampilkan kelas yang ada dalam dataset
print("Kelas yang ada dalam dataset pelatihan:")
for class_name, class_index in train_generator.class_indices.items():
    print(f"Kelas {class_index}: {class_name}")

# Jumlah kelas
num_classes = len(train_generator.class_indices)
print(f"Jumlah kelas: {num_classes}")

# Pengembangan Model MobileNetV2
input_tensor = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))
base_model = MobileNetV2(input_tensor=input_tensor, include_top=False, weights='imagenet')

# Anda bisa melanjutkan kode model dengan base_model ini sesuai kebutuhan.

import os

# Fungsi untuk menghitung jumlah gambar di setiap folder
def count_images_in_folder(folder_path):
    return len([f for f in os.listdir(folder_path) if f.endswith(('jpg', 'png', 'jpeg'))])

# Fungsi untuk menghitung total gambar untuk setiap kelas
def count_class_images(base_path, class_name):
    # Path untuk masing-masing subfolder train, validation, dan test
    train_folder = os.path.join(base_path, 'Train', class_name)
    valid_folder = os.path.join(base_path, 'Validation', class_name)
    test_folder = os.path.join(base_path, 'Test', class_name)

    # Menghitung gambar di setiap folder
    train_images = count_images_in_folder(train_folder)
    valid_images = count_images_in_folder(valid_folder)
    test_images = count_images_in_folder(test_folder)

    # Mengembalikan total gambar per kelas
    total_images = train_images + valid_images + test_images
    return train_images, valid_images, test_images, total_images

# Path ke direktori dataset Anda
DATA_DIR = '/content/drive/MyDrive/Firman/Batik Nusantara'

# Daftar nama kelas yang ada sesuai dengan penulisan kelas batik
class_names = [
    'batik-bali', 'batik-betawi', 'batik-celup', 'batik-cendrawasih', 'batik-ceplok',
    'batik-dayak', 'batik-geblek renteng', 'batik-insang', 'batik-kawung', 'batik-lasem',
    'batik-lontara', 'batik-megamendung', 'batik-pala', 'batik-parang', 'batik-poleng',
    'batik-sidomukti', 'batik-tambal', 'batik-toraja', 'batik-truntum'
]

# Iterasi untuk setiap kelas dan tampilkan jumlah gambar
for idx, class_name in enumerate(class_names):
    train, valid, test, total = count_class_images(DATA_DIR, class_name)
    print(f"Kelas {idx}: {class_name.capitalize()}:")
    print(f"  Train: {train} images")
    print(f"  Validation: {valid} images")
    print(f"  Test: {test} images")
    print(f"  Total: {total} images")
    print("-" * 40)

"""#Pre-Proccessing"""

# Konfigurasi
DATA_DIR = '/content/drive/MyDrive/Firman/Batik Nusantara'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
BATCH_SIZE = 32
GPU_DEVICE = '/GPU:0'

train_datagen = ImageDataGenerator(
   preprocessing_function=preprocess_input,
    rotation_range=40,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
)

val_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
)

# Membuat generator untuk data pelatihan
train_generator = train_datagen.flow_from_directory(
    directory=os.path.join(DATA_DIR, 'Train'),
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    color_mode='rgb',
    class_mode='categorical',
    batch_size=BATCH_SIZE,
    shuffle=True,
    seed=123
)

# Membuat generator untuk data validasi
validation_generator = val_datagen.flow_from_directory(
    directory=os.path.join(DATA_DIR, 'Validation'),
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    color_mode='rgb',
    class_mode='categorical',
    batch_size=BATCH_SIZE,
    shuffle=False,
    seed=123
)

# Membuat generator untuk data pengujian
test_generator = val_datagen.flow_from_directory(
    directory=os.path.join(DATA_DIR, 'Test'),
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    color_mode='rgb',
    class_mode='categorical',
    batch_size=BATCH_SIZE,
    shuffle=False,
    seed=123
)

# Jumlah kelas
num_classes = len(train_generator.class_indices)

"""#Pengembangan Model MobileNetV2"""

from tensorflow.keras import Input
num_classes = train_generator.num_classes

input_tensor = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))
base_model = MobileNetV2(input_tensor=input_tensor,
                         include_top=False,
                         weights='imagenet')

"""#Pengujian Hyperparameter

##Learning Rate

###Learning Rate 1e-2
"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###Learning rate 1e-3"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###Learning Rate 1e-4"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

for layer in base_model.layers:
    layer.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu')(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=35,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

for layer in base_model.layers:
    layer.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu')(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss validation(Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy validation(Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss TEST(Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy TEST(Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###Learning Rate Ie-5"""

for layer in base_model.layers:
    layer.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Batch Size

###32
"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###64"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Tuning Layer Terakhir

###Membuka 5 Layer terakhir
"""

for layer in base_model.layers[:-5]:
    layer.trainable = False
for layer in base_model.layers[-5:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###Membuka 10 Layer terakhir"""

for layer in base_model.layers[:-10]:
    layer.trainable = False
for layer in base_model.layers[-10:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###Membuka 20 Layer terakhir"""

for layer in base_model.layers[:-20]:
    layer.trainable = False
for layer in base_model.layers[-20:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')



"""###Membuka 30 Layer terakhir"""

for layer in base_model.layers[:-30]:
    layer.trainable = False
for layer in base_model.layers[-30:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###Membuka 38 Layer terakhir"""

for layer in base_model.layers[:-38]:
    layer.trainable = False
for layer in base_model.layers[-38:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###Membuka 40 Layer terakhir"""

for layer in base_model.layers[:-40]:
    layer.trainable = False
for layer in base_model.layers[-40:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')



"""###Membuka 50 Layer terakhir"""

for layer in base_model.layers[:-50]:
    layer.trainable = False
for layer in base_model.layers[-50:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Epoch

###30
"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=30,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###50

###100
"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=100,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###150"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=150,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###200"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=200,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""###250

"""

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=250,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""#Kombinasi Hyperparrameter

##Best Model
"""

for layer in base_model.layers[:-38]:
    layer.trainable = False
for layer in base_model.layers[-38:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=250,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, f1_score, recall_score, precision_score
import numpy as np

# Prediksi pada data pengujian
Y_pred = model.predict(test_generator)
y_pred = np.argmax(Y_pred, axis=1)

# Menghitung Confusion Matrix
cm = confusion_matrix(test_generator.classes, y_pred)

# Plot Confusion Matrix menggunakan Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(),
            yticklabels=test_generator.class_indices.keys())
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Laporan Klasifikasi (F1-Score, Precision, Recall)
print('Classification Report:')
print(classification_report(test_generator.classes, y_pred, target_names=test_generator.class_indices.keys()))

# Menghitung F1-Score, Precision, dan Recall secara keseluruhan (macro average)
f1 = f1_score(test_generator.classes, y_pred, average='macro')
recall = recall_score(test_generator.classes, y_pred, average='macro')
precision = precision_score(test_generator.classes, y_pred, average='macro')

print(f'Overall F1-Score: {f1:.2f}')
print(f'Overall Recall: {recall:.2f}')
print(f'Overall Precision: {precision:.2f}')

# Menyimpan model dalam format .keras
model.save('Tabulasi1_74%.keras')
print("Model telah disimpan sebagai '/content/drive/MyDrive/SKRIPSI FIRMAN/Tabulasi1 90%_2.keras'")

import tensorflow as tf
from tensorflow.keras.models import load_model
import ipywidgets as widgets
from IPython.display import display, clear_output
from PIL import Image
import numpy as np
import io
from google.colab import files  # Ini untuk Google Colab

# Memuat model yang sudah dilatih
model_path = "/content/drive/MyDrive/Firman/model_91%.keras"
model = load_model(model_path)

# Daftar kelas (sesuaikan dengan kelas yang Anda miliki)
class_names = [
    'batik-bali', 'batik-betawi', 'batik-celup', 'batik-cendrawasih', 'batik-ceplok',
    'batik-dayak', 'batik-geblek renteng', 'batik-insang', 'batik-kawung', 'batik-lasem',
    'batik-lontara', 'batik-megamendung', 'batik-pala', 'batik-parang', 'batik-poleng',
    'batik-sidomukti', 'batik-tambal', 'batik-toraja', 'batik-truntum'
]

# Fungsi untuk memprediksi kelas gambar
def predict_image(image):
    img = image.resize((224, 224))  # Resize gambar sesuai input model
    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalisasi
    img_array = np.expand_dims(img_array, axis=0)  # Tambahkan batch dimension

    predictions = model.predict(img_array)
    predicted_index = np.argmax(predictions[0])
    predicted_class = class_names[predicted_index]
    confidence = predictions[0][predicted_index]
    return predicted_class, confidence

# Fungsi untuk mengunggah dan menampilkan prediksi
def upload_and_predict():
    uploaded = files.upload()
    for fn in uploaded.keys():
        # Membaca file gambar
        img = Image.open(io.BytesIO(uploaded[fn]))

        # Menampilkan UI dengan gambar dan prediksi
        display_prediction_ui(img)

# Fungsi untuk menampilkan hasil prediksi dengan widget
def display_prediction_ui(image):
    # Bersihkan output sebelumnya
    clear_output()

    # Membuat widget untuk menampilkan gambar
    image_widget = widgets.Image(value=image_to_bytes(image), format='png', width=220, height=220)

    # Prediksi dan confidence
    predicted_class, confidence = predict_image(image)

    # Membuat label prediksi dengan gaya
    prediction_text = widgets.HTML(
        value=f"<h3 style='color: #ffffff; text-align: center;'>Prediksi Kelas: <b>{predicted_class}</b></h3>"
              f"<h4 style='text-align: center; color: #ffffff;'>Confidence: {confidence:.2f}</h4>",
        layout=widgets.Layout(margin='10px')
    )

    # Menambahkan judul dan tata letak
    title = widgets.HTML(
        value="<h2 style='text-align: center; color: #ffffff;'>Klasifikasi Gambar Batik</h2>",
        layout=widgets.Layout(margin='20px 0')
    )

    description = widgets.HTML(
        value="<p style='text-align: center; color: #ffffff;'>Unggah gambar batik untuk memprediksi kelas dan melihat tingkat kepercayaan model.</p>",
        layout=widgets.Layout(margin='10px 0')
    )

    # Kotak layout untuk gambar dan prediksi
    image_display_box = widgets.VBox([image_widget, prediction_text], layout=widgets.Layout(
        border='2px solid #ffffff', padding='15px', width='270px', align_items='center', background_color='#ffffff', margin='auto'))

    # Menampilkan UI secara keseluruhan dengan tata letak sentral dan latar belakang putih
    ui_layout = widgets.VBox([title, description, image_display_box, upload_button], layout=widgets.Layout(
        align_items='center', padding='10px', background_color='#ffffff', border='3px solid #ffffff', width='320px', margin='auto'))

    display(ui_layout)

# Fungsi untuk konversi gambar ke bytes
def image_to_bytes(img):
    with io.BytesIO() as output:
        img.save(output, format="PNG")
        return output.getvalue()

# Tombol untuk memulai proses unggah dan prediksi
upload_button = widgets.Button(
    description="Unggah Gambar Batik",
    button_style='primary',  # Warna tombol biru
    icon='upload',
    layout=widgets.Layout(width='200px', margin='10px auto')
)
upload_button.on_click(lambda x: upload_and_predict())

# Menampilkan tombol di UI
display(upload_button)

"""##Percobaan Pertama"""

for layer in base_model.layers[:-30]:
    layer.trainable = False
for layer in base_model.layers[-30:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.2)(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=100,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Percobaan kedua"""

for layer in base_model.layers[:154]:
    layer.trainable = False
for layer in base_model.layers[154:]:
    layer.trainable = True

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Melakukan fine-tuning
history_fine = model.fit(train_generator,
                         epochs=100,
                         batch_size=64,
                         validation_data=validation_generator)

combined_accuracy = history.history['accuracy'] + history_fine.history['accuracy']
combined_val_accuracy = history.history['val_accuracy'] + history_fine.history['val_accuracy']
combined_loss = history.history['loss'] + history_fine.history['loss']
combined_val_loss = history.history['val_loss'] + history_fine.history['val_loss']

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(combined_accuracy, label='Training Accuracy')
plt.plot(combined_val_accuracy, label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(combined_loss, label='Training Loss')
plt.plot(combined_val_loss, label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

val_loss, val_accuracy = model.evaluate(validation_generator)
print(f'Val Accuracy: {val_accuracy:.2f}')
print(f'Val Loss: {val_loss:.4f}')

val_loss, val_accuracy = model.evaluate(test_generator)
print(f'Val Accuracy: {val_accuracy:.2f}')
print(f'Val Loss: {val_loss:.4f}')

"""##Percobaan ketiga

---

Bukaan layer 20, Learning rate 0.001, epoch 50
"""

for layer in base_model.layers[:-20]:
    layer.trainable = False
for layer in base_model.layers[-20:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=50,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Percobaan keempat"""

for layer in base_model.layers[:-38]:
    layer.trainable = False
for layer in base_model.layers[-38:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=150,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Percobaan kelima"""

for layer in base_model.layers[:-15]:
    layer.trainable = False
for layer in base_model.layers[-15:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=70,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Percobaan keenam"""

model = tf.keras.models.load_model('/content/sample_data/model_batch64_66%.keras')

for layer in base_model.layers[:-5]:
    layer.trainable = False
for layer in base_model.layers[-5:]:
    layer.trainable = True

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Melakukan fine-tuning
history2 = model.fit(train_generator,
                         epochs=100,
                         batch_size=64,
                         validation_data=validation_generator)

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history2.history['accuracy'], label='Training Accuracy')
plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history2.history['loss'], label='Training Loss')
plt.plot(history2.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Percobaan ketujuh"""

from tensorflow.keras.regularizers import l2
for layer in base_model.layers[:-10]:
    layer.trainable = False
for layer in base_model.layers[-10:]:
    layer.trainable = True

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=input_tensor, outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=40,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Percobaan kedelapan"""

for layer in base_model.layers[:-39]:
    layer.trainable = False
for layer in base_model.layers[-39:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=250,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Percobaan kesembilan"""

for layer in base_model.layers[:-48]:
    layer.trainable = False
for layer in base_model.layers[-48:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=250,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

"""##Percobaan kesepuluh"""

for layer in base_model.layers[:-50]:
    layer.trainable = False
for layer in base_model.layers[-50:]:
    layer.trainable = True

from tensorflow.keras.regularizers import l2
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu', kernel_regularizer=l2(1e-3))(x)
x = BatchNormalization()(x)
x = Dropout(0.8)(x)
output_layer = Dense(num_classes, activation='softmax')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=output_layer)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

with tf.device(GPU_DEVICE):
    history1 = model.fit(
        train_generator,
        epochs=250,
        validation_data=validation_generator,
    )

# Plot hasil training
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'], label='Training Accuracy')
plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'], label='Training Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

test_loss, test_accuracy = model.evaluate(validation_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')

test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss (Sebelum Fine-Tuning): {test_loss:.4f}')
print(f'Test Accuracy (Sebelum Fine-Tuning): {test_accuracy:.2f}')